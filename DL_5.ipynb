{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0APr_9YvWqh",
        "outputId": "c2581ef7-6192-46ab-c924-1822260fc1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 2.9385616779327393\n",
            "Epoch 2/100, Loss: 2.9112191200256348\n",
            "Epoch 3/100, Loss: 2.8844497203826904\n",
            "Epoch 4/100, Loss: 2.85823655128479\n",
            "Epoch 5/100, Loss: 2.832562208175659\n",
            "Epoch 6/100, Loss: 2.807410717010498\n",
            "Epoch 7/100, Loss: 2.782766103744507\n",
            "Epoch 8/100, Loss: 2.7586147785186768\n",
            "Epoch 9/100, Loss: 2.7349419593811035\n",
            "Epoch 10/100, Loss: 2.7117342948913574\n",
            "Epoch 11/100, Loss: 2.688978910446167\n",
            "Epoch 12/100, Loss: 2.666663408279419\n",
            "Epoch 13/100, Loss: 2.644775867462158\n",
            "Epoch 14/100, Loss: 2.623304843902588\n",
            "Epoch 15/100, Loss: 2.6022391319274902\n",
            "Epoch 16/100, Loss: 2.5815672874450684\n",
            "Epoch 17/100, Loss: 2.561278820037842\n",
            "Epoch 18/100, Loss: 2.5413639545440674\n",
            "Epoch 19/100, Loss: 2.5218119621276855\n",
            "Epoch 20/100, Loss: 2.5026135444641113\n",
            "Epoch 21/100, Loss: 2.4837594032287598\n",
            "Epoch 22/100, Loss: 2.4652390480041504\n",
            "Epoch 23/100, Loss: 2.447044849395752\n",
            "Epoch 24/100, Loss: 2.4291672706604004\n",
            "Epoch 25/100, Loss: 2.411597728729248\n",
            "Epoch 26/100, Loss: 2.394327163696289\n",
            "Epoch 27/100, Loss: 2.3773481845855713\n",
            "Epoch 28/100, Loss: 2.360652208328247\n",
            "Epoch 29/100, Loss: 2.3442318439483643\n",
            "Epoch 30/100, Loss: 2.3280794620513916\n",
            "Epoch 31/100, Loss: 2.3121867179870605\n",
            "Epoch 32/100, Loss: 2.2965474128723145\n",
            "Epoch 33/100, Loss: 2.281153440475464\n",
            "Epoch 34/100, Loss: 2.2659988403320312\n",
            "Epoch 35/100, Loss: 2.2510764598846436\n",
            "Epoch 36/100, Loss: 2.236379861831665\n",
            "Epoch 37/100, Loss: 2.22190260887146\n",
            "Epoch 38/100, Loss: 2.2076385021209717\n",
            "Epoch 39/100, Loss: 2.1935818195343018\n",
            "Epoch 40/100, Loss: 2.1797263622283936\n",
            "Epoch 41/100, Loss: 2.166067361831665\n",
            "Epoch 42/100, Loss: 2.1525979042053223\n",
            "Epoch 43/100, Loss: 2.1393141746520996\n",
            "Epoch 44/100, Loss: 2.1262104511260986\n",
            "Epoch 45/100, Loss: 2.113281488418579\n",
            "Epoch 46/100, Loss: 2.100522756576538\n",
            "Epoch 47/100, Loss: 2.0879299640655518\n",
            "Epoch 48/100, Loss: 2.075498342514038\n",
            "Epoch 49/100, Loss: 2.063223361968994\n",
            "Epoch 50/100, Loss: 2.0511016845703125\n",
            "Epoch 51/100, Loss: 2.039128303527832\n",
            "Epoch 52/100, Loss: 2.0272998809814453\n",
            "Epoch 53/100, Loss: 2.0156123638153076\n",
            "Epoch 54/100, Loss: 2.0040626525878906\n",
            "Epoch 55/100, Loss: 1.9926468133926392\n",
            "Epoch 56/100, Loss: 1.9813616275787354\n",
            "Epoch 57/100, Loss: 1.9702037572860718\n",
            "Epoch 58/100, Loss: 1.9591701030731201\n",
            "Epoch 59/100, Loss: 1.9482578039169312\n",
            "Epoch 60/100, Loss: 1.9374641180038452\n",
            "Epoch 61/100, Loss: 1.926786184310913\n",
            "Epoch 62/100, Loss: 1.9162209033966064\n",
            "Epoch 63/100, Loss: 1.9057658910751343\n",
            "Epoch 64/100, Loss: 1.8954193592071533\n",
            "Epoch 65/100, Loss: 1.8851776123046875\n",
            "Epoch 66/100, Loss: 1.8750393390655518\n",
            "Epoch 67/100, Loss: 1.865001916885376\n",
            "Epoch 68/100, Loss: 1.8550636768341064\n",
            "Epoch 69/100, Loss: 1.8452221155166626\n",
            "Epoch 70/100, Loss: 1.835474967956543\n",
            "Epoch 71/100, Loss: 1.8258211612701416\n",
            "Epoch 72/100, Loss: 1.816258192062378\n",
            "Epoch 73/100, Loss: 1.8067848682403564\n",
            "Epoch 74/100, Loss: 1.797398567199707\n",
            "Epoch 75/100, Loss: 1.788098692893982\n",
            "Epoch 76/100, Loss: 1.7788833379745483\n",
            "Epoch 77/100, Loss: 1.7697505950927734\n",
            "Epoch 78/100, Loss: 1.7606992721557617\n",
            "Epoch 79/100, Loss: 1.7517282962799072\n",
            "Epoch 80/100, Loss: 1.7428357601165771\n",
            "Epoch 81/100, Loss: 1.7340205907821655\n",
            "Epoch 82/100, Loss: 1.7252817153930664\n",
            "Epoch 83/100, Loss: 1.7166175842285156\n",
            "Epoch 84/100, Loss: 1.7080272436141968\n",
            "Epoch 85/100, Loss: 1.699509620666504\n",
            "Epoch 86/100, Loss: 1.6910632848739624\n",
            "Epoch 87/100, Loss: 1.682687759399414\n",
            "Epoch 88/100, Loss: 1.6743814945220947\n",
            "Epoch 89/100, Loss: 1.6661434173583984\n",
            "Epoch 90/100, Loss: 1.657973289489746\n",
            "Epoch 91/100, Loss: 1.649869680404663\n",
            "Epoch 92/100, Loss: 1.6418319940567017\n",
            "Epoch 93/100, Loss: 1.6338589191436768\n",
            "Epoch 94/100, Loss: 1.6259498596191406\n",
            "Epoch 95/100, Loss: 1.6181042194366455\n",
            "Epoch 96/100, Loss: 1.610321283340454\n",
            "Epoch 97/100, Loss: 1.602599859237671\n",
            "Epoch 98/100, Loss: 1.5949394702911377\n",
            "Epoch 99/100, Loss: 1.5873396396636963\n",
            "Epoch 100/100, Loss: 1.5797990560531616\n",
            "tensor([[-4.3359e-02,  7.2047e-01,  2.2889e-01, -6.4909e-01, -8.7576e-01,\n",
            "          1.2558e+00, -1.2040e+00, -8.7920e-01,  3.1085e-01, -6.4433e-01,\n",
            "         -9.6899e-01,  6.4889e-01,  5.0418e-01,  1.2015e+00,  9.8753e-01,\n",
            "         -4.7908e-01, -1.7064e+00,  1.0199e+00,  1.4512e+00,  1.2382e+00,\n",
            "         -1.1662e+00,  2.0876e-03, -1.6950e+00, -3.5740e-01,  7.2872e-01,\n",
            "          8.1625e-01, -1.2742e+00, -9.6964e-01,  8.9004e-01, -5.8683e-01,\n",
            "          1.1480e-01,  7.2808e-01, -2.9664e-01,  3.5599e-02, -1.5585e+00,\n",
            "          8.6554e-01, -6.1828e-01, -3.7390e-01, -1.1262e+00, -4.4100e-01,\n",
            "         -7.9837e-01,  1.1160e+00,  2.2353e-01, -7.5737e-01, -1.1316e+00,\n",
            "          1.5017e+00, -3.8040e-01, -2.9510e-01, -8.6104e-01,  1.6637e+00,\n",
            "          5.3824e-01, -1.1844e+00,  7.5450e-01, -2.4141e-01, -7.7198e-01,\n",
            "         -2.4920e-01, -1.3700e+00, -1.6538e-01, -1.1081e+00, -1.3039e+00,\n",
            "          3.1878e-02,  1.3884e+00, -1.6352e+00,  1.6910e+00,  1.5108e-01,\n",
            "         -1.0501e+00, -3.2810e-02,  4.0504e-01, -1.7284e+00, -5.6892e-01,\n",
            "          2.1187e+00,  3.8831e-01,  2.3684e-01,  4.0830e-01, -1.0520e+00,\n",
            "         -1.9540e-01, -5.2947e-01, -1.2717e+00,  5.4388e-01, -1.2113e+00,\n",
            "         -1.2944e+00,  4.0849e-01, -5.9206e-01,  4.6226e-02,  3.2987e-01,\n",
            "          2.9940e-01,  1.7707e-01,  4.1694e-01,  9.5341e-01, -7.8542e-01,\n",
            "          2.0118e-01, -1.0236e+00,  1.6164e+00,  8.2146e-02, -8.9827e-01,\n",
            "          1.7202e-01, -2.4936e+00, -5.4845e-01, -2.3449e-01, -4.8791e-01],\n",
            "        [-2.3277e+00, -9.8411e-01,  7.8553e-01,  2.5428e-01, -1.8194e+00,\n",
            "          9.9380e-01, -3.2347e-01, -2.0850e-01, -3.1874e-01, -2.1694e+00,\n",
            "          1.8261e+00,  3.9315e-01,  6.2245e-01,  9.9841e-01, -2.4339e-01,\n",
            "          3.6694e-01,  9.4242e-01,  1.5499e+00, -1.1102e+00,  1.5394e+00,\n",
            "          6.3052e-01,  7.8665e-01,  5.0110e-01, -3.7865e-01,  8.2697e-01,\n",
            "          5.7952e-02,  4.4095e-01,  2.1726e-01, -3.3457e-01,  1.0809e+00,\n",
            "          4.0361e-02, -2.9505e-01, -3.7264e-02, -5.8627e-01, -6.3432e-02,\n",
            "          4.1005e-01,  6.6725e-01,  2.3795e-01,  1.7319e+00,  3.3876e-01,\n",
            "         -4.7379e-01, -4.0479e-02,  1.3122e+00, -7.0006e-01, -1.2126e+00,\n",
            "         -1.8574e+00, -8.1953e-03,  3.8883e-01,  1.1246e-01, -7.1383e-01,\n",
            "          7.3263e-01,  1.7091e+00,  1.4212e+00, -1.8176e+00,  6.1102e-01,\n",
            "          1.3062e+00, -1.0280e+00,  3.9380e-01, -1.0439e+00, -9.9656e-01,\n",
            "          9.6339e-01,  4.8910e-01, -1.7125e+00,  1.8254e-01,  7.0754e-01,\n",
            "          2.5307e-01,  9.0378e-01,  4.4262e-01, -1.4198e+00, -4.7482e-01,\n",
            "          4.1687e-01,  1.8181e-01,  7.8040e-02, -2.5634e+00,  9.1090e-01,\n",
            "         -4.2720e-02, -1.4223e+00, -4.5453e-01, -9.5441e-01, -1.5911e+00,\n",
            "         -3.7547e-01,  1.2959e+00,  1.6215e-01, -2.2542e-01, -5.0832e-01,\n",
            "          1.6000e+00,  4.7808e-01,  1.0530e+00,  3.6838e-01,  1.5515e+00,\n",
            "         -4.4495e-01, -8.4189e-01,  4.6828e-02, -1.0110e-01,  1.6250e+00,\n",
            "          2.1610e-01, -1.9070e-01,  3.1817e-01,  9.8797e-01, -4.3812e-01],\n",
            "        [ 3.8984e-01, -6.6712e-01, -1.7034e-01, -4.0008e-01,  5.8935e-01,\n",
            "         -9.2646e-01,  9.3168e-01, -1.7670e+00, -7.1396e-01,  3.3802e-01,\n",
            "          1.7528e+00, -1.3530e+00, -3.7023e-01,  6.6260e-01, -7.7114e-01,\n",
            "         -8.5921e-03, -7.8771e-01, -3.6868e-01, -3.9199e-01, -4.0518e-01,\n",
            "         -9.4360e-01, -5.0930e-01, -7.1777e-01, -7.5121e-01, -1.2138e+00,\n",
            "         -8.4844e-01, -1.4707e+00, -1.4984e+00,  6.9747e-01, -1.0698e-01,\n",
            "          1.0191e+00,  6.7005e-02, -7.4662e-01, -2.1163e-01, -1.1752e+00,\n",
            "         -1.3283e-01, -7.6008e-01, -2.4582e-01,  5.5986e-01,  6.5653e-01,\n",
            "          1.1050e+00,  4.9489e-01, -1.4236e+00, -1.2578e+00, -7.2258e-01,\n",
            "         -1.2996e+00,  5.9740e-01,  1.5962e+00, -1.5642e+00,  2.6574e-03,\n",
            "         -6.4009e-01,  1.0915e-01, -1.8680e+00,  1.3540e+00, -1.8473e-01,\n",
            "         -2.0492e+00,  2.2921e-01,  1.3542e-02,  9.0349e-01, -1.3546e+00,\n",
            "          2.2382e-01, -1.8591e-01,  2.5486e-01,  8.1677e-01, -4.2154e-01,\n",
            "         -1.6025e+00,  8.6951e-01, -2.0123e-01,  1.2516e+00, -3.0378e-02,\n",
            "          1.5572e+00, -8.7629e-01, -4.7957e-01, -2.6078e-01,  1.3990e+00,\n",
            "         -2.2990e+00, -8.4794e-01, -2.6931e-01, -1.7212e+00,  4.7936e-01,\n",
            "          1.4722e+00,  2.2661e+00, -3.0784e-01, -3.9531e-01, -3.0123e-01,\n",
            "         -1.0543e+00, -8.3116e-02,  5.6982e-01, -4.6415e-01,  3.1802e-01,\n",
            "          3.9340e-01,  9.8106e-01,  8.9299e-02, -2.3121e+00, -4.7014e-01,\n",
            "         -7.7186e-02,  4.8420e-03, -8.5396e-01,  1.2895e+00, -8.9533e-01],\n",
            "        [-6.4242e-01, -5.4422e-01,  1.3908e+00,  3.0025e-01, -3.5728e-01,\n",
            "         -8.0794e-01,  1.4882e+00, -1.6931e-01,  8.3115e-02,  3.1918e+00,\n",
            "          8.5471e-02, -1.0188e+00, -2.0103e-02, -7.8428e-01,  4.4826e-01,\n",
            "         -1.0877e+00, -6.9794e-01, -7.6547e-01, -7.0040e-01,  3.0128e-01,\n",
            "          3.4568e-01,  6.3396e-01,  5.9549e-01, -3.6486e-01, -4.6471e-01,\n",
            "          2.2191e+00,  1.1595e+00,  1.3253e+00, -6.2721e-01,  9.8188e-01,\n",
            "          1.5905e-01, -2.1999e+00,  1.6993e+00,  1.1539e+00,  6.7525e-01,\n",
            "         -5.4862e-02,  6.9697e-01, -2.5503e-01, -1.0827e-01,  4.3810e-01,\n",
            "          3.7898e-01,  1.0080e+00, -1.5973e+00,  2.5982e+00,  2.0885e+00,\n",
            "          1.7574e+00, -7.3343e-01,  6.0940e-01,  6.9413e-01, -2.0920e-01,\n",
            "          1.0117e+00, -6.8289e-01,  3.2044e-01,  1.2104e+00,  1.1692e+00,\n",
            "         -4.1435e-01,  5.0589e-01, -3.7501e-01,  4.0424e-01, -5.2753e-02,\n",
            "         -4.4754e-01,  2.1073e-01, -1.3987e+00,  1.4174e+00,  8.5863e-01,\n",
            "          2.3658e-01, -5.1947e-01, -1.1872e+00,  8.6742e-01,  1.1016e+00,\n",
            "         -1.0445e+00, -9.8989e-01,  4.1012e-01, -1.0232e+00, -4.5807e-01,\n",
            "          1.4213e-01, -2.5519e-01, -9.0507e-01, -3.3877e-01, -5.5743e-01,\n",
            "          1.3510e-01, -3.9137e-01,  5.8739e-01,  9.7684e-02,  6.2864e-02,\n",
            "         -3.0761e-01, -4.9063e-02, -5.6010e-01, -1.7945e+00, -3.7851e-01,\n",
            "          1.6082e+00,  9.6465e-01, -1.0890e+00, -6.3131e-01,  4.9423e-01,\n",
            "          5.0450e-02,  2.9822e-01, -2.0053e-01, -1.6170e+00,  3.9243e-01],\n",
            "        [ 7.7256e-01,  8.6520e-01,  1.1657e+00, -4.6193e-01, -6.1211e-01,\n",
            "          3.0358e-01, -5.3830e-01,  7.7896e-01, -5.9129e-01, -8.0310e-01,\n",
            "          1.8524e+00, -8.4412e-01, -5.0142e-01,  1.4297e-01,  3.6602e-01,\n",
            "         -1.0025e+00, -8.1431e-01,  1.4225e+00,  3.7666e-01,  1.1837e+00,\n",
            "         -5.2772e-01, -1.7210e-01,  1.5404e+00, -2.6276e-01, -1.0568e+00,\n",
            "         -3.8023e-01, -9.8451e-01,  2.3173e+00,  1.1762e+00, -9.0031e-01,\n",
            "          4.8925e-02,  1.0765e+00, -2.5243e-02, -1.3170e+00, -8.9110e-01,\n",
            "         -1.2226e+00, -1.0109e+00, -4.0671e-01,  4.3512e-01, -2.5415e-01,\n",
            "         -1.1736e+00, -1.4187e+00,  1.2412e+00, -9.7063e-01,  1.5779e-02,\n",
            "         -8.1119e-01,  1.8142e+00,  4.7993e-01, -1.4626e+00,  7.5722e-02,\n",
            "         -1.4803e+00, -1.3577e-01, -5.6753e-01, -2.8777e-01, -2.9221e-01,\n",
            "         -1.2757e+00,  6.9337e-01, -1.8085e+00,  1.0954e+00,  4.4871e-03,\n",
            "          1.2683e+00, -1.2799e+00,  2.6899e-01,  1.8287e-01,  1.9305e+00,\n",
            "         -7.7272e-01, -4.5797e-01,  1.6880e+00, -4.6108e-02,  8.0713e-01,\n",
            "          4.3916e-01,  9.6237e-01, -2.1387e-01,  3.6225e-01,  2.8211e-01,\n",
            "         -8.7681e-01, -9.6725e-02,  1.0349e-01, -2.1822e-01,  1.9384e-01,\n",
            "          1.1259e+00,  2.4680e+00,  1.4988e+00,  8.0906e-01, -5.6193e-01,\n",
            "         -6.9165e-01,  8.3843e-01,  2.1217e-01,  6.9328e-01,  6.0542e-01,\n",
            "         -9.5923e-02, -6.3304e-01,  1.3360e-01, -1.1194e+00, -2.9291e-01,\n",
            "          2.3024e-01, -8.9565e-02, -7.1233e-01, -1.2886e+00,  4.1103e-02],\n",
            "        [ 1.2928e+00, -5.5712e-01,  1.2303e-01, -3.5702e-02,  1.5136e+00,\n",
            "         -4.7761e-02,  3.0709e-01,  3.2133e-03,  3.3952e-01,  3.9695e-01,\n",
            "         -5.6674e-01, -3.4286e+00,  2.6538e-01, -2.3979e-01, -4.4379e-01,\n",
            "          1.1504e-01, -1.0779e+00,  3.4415e-01, -4.5421e-01,  8.5290e-01,\n",
            "          1.4172e+00,  9.0673e-01, -4.7747e-01, -6.2391e-02, -6.6900e-01,\n",
            "          1.1418e+00,  4.2841e-01, -2.4899e-01,  1.2028e+00, -1.1371e-02,\n",
            "          3.0289e+00, -1.3801e+00, -9.2546e-01,  9.2769e-01, -7.4947e-01,\n",
            "          7.3574e-01, -9.8033e-02,  7.4199e-01, -4.5236e-01,  9.0730e-01,\n",
            "          6.8796e-01,  3.5641e-01,  4.7691e-01,  9.8742e-02,  1.2751e+00,\n",
            "          6.7952e-01, -1.6005e-01,  6.2424e-01,  4.9728e-01,  1.5682e+00,\n",
            "          6.2623e-02, -5.9515e-01, -5.4213e-01,  7.9507e-01, -8.7635e-01,\n",
            "         -1.0255e-01, -3.6309e-01, -2.5684e-01,  5.1425e-02, -2.1004e+00,\n",
            "          6.9315e-02,  1.3486e-01, -2.1270e-01,  1.5451e+00,  6.4950e-01,\n",
            "          6.4060e-01, -3.2903e-01, -9.9580e-01,  3.7268e-01,  1.0784e+00,\n",
            "          2.2776e-01, -7.3721e-03,  1.2871e+00,  7.1204e-01,  7.1487e-01,\n",
            "         -6.5619e-01, -1.1131e+00,  2.7535e-01,  7.2825e-01, -1.0812e+00,\n",
            "          6.6300e-02,  1.3639e-01, -6.9537e-01, -1.5702e+00, -5.1631e-01,\n",
            "          7.9855e-01,  1.4727e-01, -1.1756e+00,  1.1618e+00,  1.3974e+00,\n",
            "          4.2972e-01, -3.2916e-01, -4.8837e-01,  4.4043e-01,  3.1260e-03,\n",
            "         -4.3909e-01, -1.1282e+00,  2.5535e-01, -7.7577e-01,  6.5518e-01],\n",
            "        [-7.8808e-01, -7.0593e-01, -4.4117e-02,  1.1359e+00,  6.0361e-01,\n",
            "         -7.8340e-01,  1.7259e+00,  1.9830e+00,  1.5253e-01, -9.1209e-01,\n",
            "          8.6536e-01,  7.5056e-02, -8.5935e-01,  1.7809e-01,  9.4752e-01,\n",
            "         -2.0118e+00,  8.4127e-01,  1.4654e+00, -1.3488e+00, -9.0282e-01,\n",
            "         -1.4471e+00, -1.4398e+00,  1.1997e+00, -5.7151e-01, -2.0286e-01,\n",
            "         -7.7750e-01,  7.6175e-01,  5.3650e-01, -1.7705e+00, -2.8803e-01,\n",
            "          4.5041e-01,  6.2195e-01,  1.4619e+00, -1.8395e+00,  1.0298e-01,\n",
            "         -2.3091e+00, -6.2877e-01,  1.6299e+00,  1.7262e+00, -1.6312e+00,\n",
            "         -1.5630e-01, -6.4821e-01,  1.4504e+00,  9.2402e-01, -6.1941e-01,\n",
            "          5.6041e-01,  2.2231e-01, -6.8056e-01,  7.3326e-01,  5.6259e-01,\n",
            "          1.4231e+00, -1.2237e-01,  1.6865e+00, -1.1658e-01,  6.2999e-01,\n",
            "         -1.1929e+00,  2.3129e-01,  9.2109e-01,  6.2803e-01,  5.0328e-01,\n",
            "         -1.0722e+00,  1.8304e-01, -5.4335e-01,  1.6065e+00,  1.5763e-01,\n",
            "          6.6829e-01, -1.1662e+00,  1.4284e+00,  1.8731e-01, -2.9749e-01,\n",
            "         -5.9678e-01,  2.1646e+00,  1.7840e+00,  8.7466e-01,  2.1166e-01,\n",
            "          5.4136e-01,  6.9612e-01, -6.4380e-01,  9.7076e-01,  8.8970e-01,\n",
            "          2.2334e+00,  1.3268e+00,  6.0548e-01, -1.1450e-01, -7.6801e-01,\n",
            "         -3.4415e-01, -3.0387e-01, -7.9451e-01,  7.9666e-02,  1.4600e+00,\n",
            "          2.8193e+00, -1.1846e+00,  1.4955e+00,  3.7478e-01,  5.3469e-01,\n",
            "         -7.1904e-01,  4.2066e-01,  2.0103e+00,  1.9031e+00, -1.1540e-01],\n",
            "        [-7.8594e-02,  3.0088e-01,  1.9126e+00, -2.3445e-01, -1.0819e+00,\n",
            "          6.5146e-01, -5.3231e-01,  3.9453e-01,  6.1678e-01, -1.0884e+00,\n",
            "         -4.3711e-03,  1.3677e+00,  5.4891e-01, -1.3765e+00, -4.4564e-01,\n",
            "          1.0976e+00,  5.8852e-01, -9.0383e-01, -2.6197e-02,  9.7155e-01,\n",
            "         -3.4924e-02, -1.2220e+00, -1.9779e-01,  8.9628e-01,  1.0183e+00,\n",
            "          3.2941e-01,  1.3615e+00,  7.1027e-02,  9.4825e-01, -1.6742e+00,\n",
            "         -6.3486e-01,  9.9712e-01,  7.1155e-01,  5.4704e-02, -5.3504e-01,\n",
            "         -4.4988e-01, -7.9592e-01,  1.6335e+00, -4.8136e-01,  8.8936e-03,\n",
            "          2.0132e+00, -1.4583e+00,  7.4907e-02,  3.3252e-01,  1.4728e-01,\n",
            "         -1.9191e-01,  2.5943e+00, -1.1013e+00, -1.8067e-01, -1.2660e-01,\n",
            "          1.3486e+00,  7.5364e-01,  6.9095e-02,  9.6750e-01,  1.8354e+00,\n",
            "          1.9332e+00, -8.4123e-02,  7.3134e-01, -1.4438e+00,  8.9093e-01,\n",
            "          1.8859e+00, -1.5590e+00, -3.0626e-01, -3.8055e-01,  2.9176e+00,\n",
            "          4.9522e-01,  1.0786e+00,  1.0288e+00, -1.2590e+00, -2.5032e-01,\n",
            "          3.8397e-02, -9.7424e-02, -3.3297e-01, -1.2442e+00,  1.7115e+00,\n",
            "          9.9947e-01,  8.3829e-02,  1.0889e+00, -1.4239e+00, -2.3425e-01,\n",
            "         -2.0948e-01, -3.7146e-01, -1.7759e+00,  1.0473e+00,  5.1655e-01,\n",
            "          7.5798e-01,  1.0858e+00,  2.4770e-01,  5.9333e-01,  9.5290e-01,\n",
            "         -3.8486e-01, -3.0769e-01,  3.4633e-02, -1.5172e+00, -1.5310e-01,\n",
            "          1.8760e+00,  5.6078e-01, -2.0557e+00,  4.3937e-01, -6.5632e-01],\n",
            "        [ 1.2250e+00, -1.3332e+00,  1.0350e+00, -7.8755e-01, -4.0185e-01,\n",
            "          2.1985e-01,  1.6829e+00, -6.6829e-01, -1.1040e+00,  4.1883e-01,\n",
            "         -9.2091e-01, -1.1475e+00,  1.2000e+00,  4.1400e-01, -2.2106e+00,\n",
            "          8.1024e-01,  4.0979e-01,  3.2063e-01,  1.3434e+00,  5.8310e-01,\n",
            "         -1.4084e+00,  3.9995e-01,  1.0806e+00, -9.9755e-01,  6.6112e-02,\n",
            "         -9.0138e-01,  4.7735e-01,  8.1421e-01,  2.4419e-01, -1.2279e+00,\n",
            "         -1.8825e+00, -5.1881e-01, -3.0589e+00,  6.3272e-02,  1.6122e-01,\n",
            "          7.2078e-01,  5.3951e-01, -9.4762e-01,  6.9365e-01,  2.5357e+00,\n",
            "         -1.3987e+00, -1.0359e+00,  9.8071e-01,  8.6370e-01, -4.5822e-01,\n",
            "         -2.7157e-01, -3.5877e-01,  2.1998e-01, -8.8848e-01, -3.2899e+00,\n",
            "         -2.1612e+00,  1.3804e+00,  1.7677e-03, -1.4171e+00,  6.1176e-01,\n",
            "         -1.7681e+00, -1.7209e+00, -1.5758e+00,  1.9633e+00, -1.6118e+00,\n",
            "         -3.7571e-02, -6.6049e-02,  8.4153e-01,  5.7489e-01,  4.8281e-01,\n",
            "          1.3584e+00, -9.9907e-01, -4.3625e-01, -4.7239e-01,  6.8348e-02,\n",
            "          6.9317e-02, -6.8828e-01, -3.9410e-01, -4.0381e-01,  1.3574e+00,\n",
            "          1.2885e-01, -1.1566e+00,  2.5379e-01,  6.5610e-01,  7.0185e-01,\n",
            "          1.2536e+00,  1.0981e+00,  9.1080e-01, -7.8034e-02, -1.1854e+00,\n",
            "         -5.7498e-01, -5.6223e-01,  1.1265e-02,  7.7472e-01, -1.5467e+00,\n",
            "          8.7233e-01,  1.0423e+00, -2.3696e-01,  1.3113e+00,  4.2691e-02,\n",
            "         -1.0192e+00,  2.1169e+00, -5.7078e-01,  1.3603e+00, -4.3727e-01],\n",
            "        [ 8.0484e-01, -1.1545e+00, -6.6997e-02,  8.5750e-01,  1.7077e+00,\n",
            "          3.9747e-01, -6.3113e-01, -3.1662e-01,  2.4051e-01, -4.9947e-01,\n",
            "         -6.8394e-01, -1.3249e+00, -1.5205e+00,  9.9934e-01,  1.7154e+00,\n",
            "         -6.1593e-01, -1.9252e-01, -1.4635e-02,  8.1748e-01, -1.7711e+00,\n",
            "          7.7391e-01,  3.4264e-01, -1.0114e+00, -1.4575e+00,  1.7348e-01,\n",
            "         -1.6752e+00, -1.2876e+00,  1.1834e-01,  6.6367e-01, -9.4384e-01,\n",
            "          4.5723e-01, -3.4884e-01,  1.3632e+00,  1.0646e+00,  1.5546e-01,\n",
            "         -3.3452e-01, -2.1225e+00, -5.6758e-01,  3.1910e-01,  1.3600e-01,\n",
            "          1.3057e+00, -1.0089e+00, -1.5325e+00, -2.5760e-01, -1.0355e+00,\n",
            "          1.4843e-01, -7.3670e-01,  3.3411e-01, -9.2445e-02,  3.8221e-02,\n",
            "         -1.1183e-02, -4.3485e-01, -1.3572e+00,  1.0608e+00, -2.1136e+00,\n",
            "          8.8362e-01, -8.1293e-01, -5.5931e-01,  7.3777e-01,  1.5953e+00,\n",
            "          3.2633e-02,  8.2040e-02, -8.1257e-01, -2.7166e-01,  1.4257e+00,\n",
            "          6.4324e-01, -1.3019e+00,  1.3756e+00, -3.5319e-01, -1.2045e+00,\n",
            "          7.9182e-01,  1.6267e+00, -9.0264e-01,  1.5128e-01,  9.7881e-02,\n",
            "          6.2424e-01, -1.5933e-01,  3.5545e-02,  2.7865e-01, -6.5060e-01,\n",
            "          1.1993e+00, -5.6926e-02,  1.4508e-01, -2.8073e-01,  1.5571e-01,\n",
            "         -9.1282e-01,  6.7536e-01,  4.0050e-01,  3.1883e-01, -2.3046e+00,\n",
            "         -9.3630e-02, -3.6100e-01, -8.2659e-02, -1.7380e+00,  1.8075e+00,\n",
            "         -4.3367e-02,  8.2917e-01, -1.5252e+00,  1.0944e-02,  1.6482e+00]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Example corpus (replace with your own dataset)\n",
        "corpus = [\n",
        "    \"i love natural language processing\",\n",
        "    \"continuous bag of words model\",\n",
        "    # Add more sentences from your dataset\n",
        "]\n",
        "\n",
        "# Tokenize the corpus and build a vocabulary\n",
        "words = ' '.join(corpus).split()\n",
        "word_counts = Counter(words)\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Define hyperparameters\n",
        "embedding_dim = 100\n",
        "context_window = 2\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "# Create training data in context-target pairs\n",
        "data = []\n",
        "for sentence in corpus:\n",
        "    words = sentence.split()\n",
        "    for target_idx in range(len(words)):\n",
        "        context = [\n",
        "            words[i] for i in range(target_idx - context_window, target_idx)\n",
        "            if i != target_idx and 0 <= i < len(words)\n",
        "        ]\n",
        "        target = words[target_idx]\n",
        "        data.append((context, target))\n",
        "\n",
        "# Define a custom dataset and dataloader\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, data, word_to_idx, context_window):\n",
        "        self.data = data\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.context_window = context_window\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        context, target = self.data[index]\n",
        "        context = [self.word_to_idx[word] for word in context]\n",
        "        target = self.word_to_idx[target]\n",
        "        return context, target\n",
        "\n",
        "def collate_fn(data):\n",
        "    # Padding sequences to the length of the longest context sequence\n",
        "    max_len = max(len(context) for context, _ in data)\n",
        "    padded_contexts = []\n",
        "    targets = []\n",
        "\n",
        "    for context, target in data:\n",
        "        padding = [0] * (max_len - len(context))\n",
        "        padded_context = context + padding\n",
        "        padded_contexts.append(padded_context)\n",
        "        targets.append(target)\n",
        "\n",
        "    return torch.tensor(padded_contexts), torch.tensor(targets)\n",
        "\n",
        "dataset = CBOWDataset(data, word_to_idx, context_window)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Define the CBOW model\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        context = self.embeddings(context).sum(dim=1)\n",
        "        output = self.linear(context)\n",
        "        return output\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CBOW(len(vocab), embedding_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for context, target in dataloader:\n",
        "        context = context.to(torch.long)\n",
        "        target = target.to(torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(context)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}')\n",
        "\n",
        "# Save the learned word embeddings\n",
        "word_embeddings = model.embeddings.weight.data\n",
        "print(word_embeddings)\n",
        "# You can now use the word embeddings for various NLP tasks\n"
      ]
    }
  ]
}